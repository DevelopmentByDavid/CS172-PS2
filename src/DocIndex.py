import argparse
import os
import sys
import time
import cProfile, pstats
import math
import re
import json
from operator import itemgetter, attrgetter, methodcaller
from nltk.stem import PorterStemmer

### GLOBALS
#inputs i.e. r 
STOP_FILE_PATH = os.path.join(os.path.dirname(__file__), "../stoplist.txt")
#outputs i.e. w
STATS_FILE_PATH = os.path.join(os.path.dirname(__file__), "../output/stats.txt")
POSTINDEX_FILE_PATH = os.path.join(os.path.dirname(__file__), "../output/posting-index.txt")
POSTINDEX_BACKUP_PATH = os.path.join(os.path.dirname(__file__), "../output/posting-index-backup.txt")
DOCUMENT_FILE_PATH = os.path.join(os.path.dirname(__file__), "../output/document-index.txt")
DOCUMENT_BACKUP_PATH = os.path.join(os.path.dirname(__file__), "../output/document-index-backup.txt")



#CLI
parser = argparse.ArgumentParser(description="Creates a posting index and document index from files specified")
parser.add_argument("-f", "--file", default = None,\
    help = "Specify file to add to the index.",\
    metavar="<filename>")
parser.add_argument("-d", "--dir", default = None,\
    help = "Specify directory to add to the index",\
    metavar="<directory>")
parser.add_argument("-s", "--stats", default = None,\
    help = "#print stats related to index creation.  #prints autogenerated file stats.txt",\
    action = "store_true")
parser.add_argument("-l", "--list", default = None, help = "#print list of files used for index.", action = "store_true")
parser.add_argument("--clear", default = None,\
    help = "Clear index and inverted index.\
    If used with --file and other commands,\
    clear will occur first then generate the new index. Generates backup files on use.",\
    action = "store_true")
parser.add_argument("--find", default = None, help = "Find term in inverted index")
parser.add_argument("--trec", default = None, help = "Add a TREC file")




#DocIndex class definition
class DocIndex:
    def __init__(self):
        # inits
        self.postingIndex = None
        self.docIndex = None
        self.stopSet = None
        self.fileList = None
        # TREC related stuff
        self.inDoc = False
        self.inText = False
        self.inDocNo = False
        self.tag = ["</doc>", "<doc>", "<text>", "</text>", "<docno>", "</docno>"]
        self.TREC = False
        self.TrecDocNo = None
        self.TrecText = []
        # loads
        self.loadAll()

    def loadAll(self):
        #print("Loading all necessary files...")
        self.loadStopList()
        self.loadPostingIndex()
        self.loadDocIndex()
    
    def stripTrec(self, word):
        if word.lower() in self.tag:
            self.TrecMap[word.lower()][0](self.TrecMap[word.lower()][1])
            return word.lower()

        if not self.inDocNo:
            return re.sub(r"\W+|(\s\s+)|(\A\Z)+|(\\0)", '', word.lower())       
        elif self.inDocNo:
            return word
        else:
            return re.sub(r"(\s\s+)|(\\0)", "", word.lower())

    def strip(self, word):
        if self.TREC:
            return self.stripTrec(word)
        else:
            return re.sub(r"\W+|(\s\s+)|(\A\Z)+|(\\0)", '', word)

    def setInDoc(self, val):
        if val == self.inDoc:
            print("ERROR: <DOC></DOC> tags are not in pairs")
            print("Unbalanced Tag: ", val)
        else:
            self.inDoc = val

    def setInDocNo(self, val):
        if val == self.inDocNo:
            print("ERROR: <DOCNO></DOCNO> tags are not in pairs")
        else:
            self.inDocNo = val

    def setInText(self, val):
        if val == self.inText:
            print("ERROR: <TEXT></TEXT> tags are not in pairs")
        else:
            self.inText = val 

    @staticmethod
    def removeNone(text):
        retVal = []
        for word in text:
            if word:
                retVal.append(word)
        return retVal

    @staticmethod
    def exists(toCheck):
        if os.path.exists(toCheck):
            return True
        #print("ERROR: \'", toCheck, "\' does not exist?")
        return False

    @staticmethod
    def notFound(name, path):
        # Error messages`
        if not os.path.exists(path):
            print("ERROR: Looked for \'", path, "\' and did not find it.")
        if not os.path.isfile(path):
            print("ERROR:", name ," file path \'", path, "\' is not a file.")

    @staticmethod
    def countTokens(token, tokens):
        count = 0
        for tok  in tokens:
            if tok == token:
                count += 1
        return count
                
    def loadStopList(self):
        if os.path.exists(STOP_FILE_PATH) and os.path.isfile(STOP_FILE_PATH):
            f = open(STOP_FILE_PATH, 'r')
            self.stopSet = f.readlines()
            temp = set()
            for n in self.stopSet:
                temp.add(re.sub(r"\s+", "", self.strip(n)))
            f.close()
            self.stopSet = temp
            return
        self.notFound("Stop List", STOP_FILE_PATH)
        #print("WARNING: Currently using no stop list!")
    
    def loadPostingIndex(self):
        if os.path.exists(POSTINDEX_FILE_PATH) and os.path.isfile(POSTINDEX_FILE_PATH):
            f = open(POSTINDEX_FILE_PATH, 'r')
            try:
                self.postingIndex = json.load(f)
            except:
                self.postingIndex = {}
            f.close()
            return
        # In the case that the file does not exist    
        #print("Did not find an index file! Creating one from scratch..")
        os.makedirs(os.path.dirname(POSTINDEX_FILE_PATH), exist_ok = True)
        f = open(POSTINDEX_FILE_PATH, "w+")
        json.dump({}, f)
        f.close()
        self.postingIndex = dict()
        
    def loadDocIndex(self):
        if os.path.exists(DOCUMENT_FILE_PATH) and os.path.isfile(DOCUMENT_FILE_PATH):
            f = open(DOCUMENT_FILE_PATH, 'r')
            try:
                self.docIndex = json.load(f)
            except:
                self.docIndex = {}
            f.close()
            return
        # In the case that the file does not exist    
        #print("Did not find an inverted index file! Creating one from scratch..")
        os.makedirs(os.path.dirname(DOCUMENT_FILE_PATH), exist_ok = True)
        f = open(DOCUMENT_FILE_PATH, "w+")
        json.dump({}, f)
        f.close()
        self.docIndex = dict()
    
    def addDirectory(self, directory):
        # #print(directory)
        if(self.exists(directory)):
            for n in os.listdir(directory):
                if n.endswith(".txt"):
                    self.addDocument(directory + "/" + n)
        
    def addDocument(self, document):
        if not self.exists(document):
            return
        if self.TREC:
            self.TrecInit()

        strippedText = []
        f = open(document, 'r')
        lines = f.readlines()
        for line in lines:
            for word in line.split():
                retWord = self.strip(word)
                if retWord:
                    strippedText.append(retWord)
        f.close()
        removeNone = self.removeNone(strippedText)
        stoppedTokens = self.tokenize(removeNone)
        docId = os.path.basename(document) #will update later if it's TREC
        if self.TREC:
            self.parseTrec(stoppedTokens)
        else:
            self.addToDocIndex(docId, len(stoppedTokens))
            self.updatePostingIndex(stoppedTokens, docId)

    def parseTrec(self, stoppedTokens):
        for token in stoppedTokens:
            self.checkToken(token)
            self.parseToken(token)
    
    def parseToken(self, token):
        # print(self.inDoc)
        if self.inDoc and self.inDocNo:
            self.TrecDocNo = token
        elif self.inDoc and self.inText:
            self.TrecText.append(token)
    
    def checkToken(self, token):
        if token in self.tag:
            self.TrecMap[token][0](self.TrecMap[token][1])
            # print(token)

        if (not self.inDoc) and self.TrecText != None and self.TrecDocNo != None:
            self.addToDocIndex(self.TrecDocNo, len(self.TrecText))
            self.updatePostingIndex(self.TrecText, self.TrecDocNo)
            self.TrecText = []
            self.TrecDocNo = None

    
    def TrecInit(self):
        self.TrecMap = {
            "<doc>": [self.setInDoc, True],
            "</doc>": [self.setInDoc, False],
            "<text>": [self.setInText, True],
            "</text>": [self.setInText, False],
            "<docno>": [self.setInDocNo, True],
            "</docno>": [self.setInDocNo, False]
        }

    def removeStopWords(self, strippedWords):
        retText = []
        for n in strippedWords:
            if not n.lower() in self.stopSet:
                retText.append(n.lower())
        return retText

    def tokenize(self, strippedWords):
        stoppedTokens = self.removeStopWords(strippedWords)
        stemmedTokens = self.applyStemming(stoppedTokens)
        return stemmedTokens
    
    def addToDocIndex(self, docId, value):
        self.docIndex[docId] = {"_id": docId, "terms": value}
        
    def write(self): 
        self.writeAll()
    
    def writeAll(self):
        self.writeDocIndex()
        self.writePostingIndex()
    
    def writeDocIndex(self):
        #print("Writing to " + DOCUMENT_FILE_PATH)
        f = open(DOCUMENT_FILE_PATH, 'w')
        json.dump(self.docIndex, f)
        f.close()
    
    def writePostingIndex(self):
        #print("Writing to " + POSTINDEX_FILE_PATH)
        f = open(POSTINDEX_FILE_PATH, 'w')
        json.dump(self.postingIndex, f)
        f.close()

    def updatePostingIndex(self, tokens, docId):
        tokenSet = set(tokens)
        for token in tokenSet:
            count = self.countTokens(token, tokens)
            countDiff = 0
            if token in self.postingIndex:
                tCountOld = self.postingIndex[token]["tCount"]
                if docId in self.postingIndex[token]["postings"]:
                    docCountOld = self.postingIndex[token]["postings"][docId]["termFreq"]
                    countDiff = count - docCountOld
                else:
                    self.postingIndex[token]["postings"][docId] = dict()
                    self.postingIndex[token]["postings"][docId]["_id"] = docId
                    countDiff = count
                self.postingIndex[token]["postings"][docId]["termFreq"] = count
                tCountNew = countDiff + tCountOld
                self.postingIndex[token]["tCount"] = tCountNew
            else:
                self.postingIndex[token] = dict()
                self.postingIndex[token]["postings"] = dict()
                self.postingIndex[token]["postings"][docId] =  dict()
                self.postingIndex[token]["postings"][docId]["termFreq"] = count
                self.postingIndex[token]["postings"][docId]["_id"] = docId
                self.postingIndex[token]["tCount"] = count
    
    def clear(self):
        #print("Clearing and backing up files...")
        #save old file first then clear current ones
        #print("Backing up " + POSTINDEX_FILE_PATH + " to " + POSTINDEX_BACKUP_PATH)
        g = open(POSTINDEX_BACKUP_PATH, "w+")
        json.dump(self.postingIndex, g)
        g.close()

        #print("Clearing " + POSTINDEX_FILE_PATH)
        f = open(POSTINDEX_FILE_PATH, "w+")
        json.dump({}, f)
        f.close()

        #print("Backing up " + DOCUMENT_FILE_PATH + " to " + DOCUMENT_BACKUP_PATH)
        g = open(DOCUMENT_BACKUP_PATH, "w+")
        json.dump(self.docIndex, g)
        g.close()

        #print("Clearing " + DOCUMENT_FILE_PATH)
        f = open(DOCUMENT_FILE_PATH, "w+")
        json.dump({}, f)
        f.close()
    
    def findTerm(self, termToFind):
        ps = PorterStemmer()
        termToFind = ps.stem(termToFind)
        if termToFind in self.postingIndex:
            return self.postingIndex[termToFind]
        return None
    
    def findDoc(self, docId):
        if docId in self.docIndex:
            return self.docIndex[docId]
        return None

    def getDocIndexSize(self):
        return len(self.docIndex)

    def applyStemming(self, tokens):
        ps = PorterStemmer()
        retText = []
        for token in tokens:
            retText.append(ps.stem(token))
        return retText

    def setTrec(self, setTo):
        self.TREC = setTo
    
    def search(self, queryString):
        results = []
        for word in queryString:
            results.append(self.findTerm(word))

def main(args):
    args = parser.parse_args(args)
    index = DocIndex()
    if args.clear != None:
        index.clear()
    if args.file != None:
        index.addDocument(args.file)
        index.write()
    if args.dir != None:
        index.addDirectory(args.dir)
        index.write()
    if args.stats != None:
        #print("TODO")
        pass
    if args.list != None:
        #print("TODO")
        pass
    if args.trec != None:
        index.setTrec(True)
        index.addDocument(args.trec)
        index.write()
        pass
    if args.find != None:
        termInfo = index.findTerm(args.find)
        if termInfo != None:
            relevantDocs = dict()
            for n in termInfo["postings"]:
                relevantDocs[n] = index.findDoc(n)
            # print("Found!")
            # print([termInfo, relevantDocs, index.getDocIndexSize()])
            return [termInfo, relevantDocs, index.getDocIndexSize()]
    return None


if __name__ == "__main__":
    main(sys.argv[1:])